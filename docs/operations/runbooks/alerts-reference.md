# üö® Alerts Reference

> **Refer√™ncia completa de alertas do Prometheus**

---

## Alert Overview

Todos os alertas s√£o definidos em `alerts.yml` e roteirizados pelo Alertmanager para notifica√ß√µes via email.

**Dashboard:** http://localhost:9093 (Alertmanager UI)
**Defini√ß√£o:** http://localhost:9090/alerts (Prometheus Alerts)

---

## Test Alerts

### Test_Always_Firing

**Severidade:** test
**Condi√ß√£o:** Sempre dispara (vector(1))
**Prop√≥sito:** Validar pipeline Prometheus ‚Üí Alertmanager ‚Üí Email

**A√ß√£o:** Ignorar - apenas para testes

---

## Infrastructure Alerts

### HighCPUUsage

**Severidade:** warning
**Servi√ßo:** node-exporter
**Condi√ß√£o:** CPU > 80% por >1 minuto
**Significado:** Host est√° com alta utiliza√ß√£o de CPU

**A√ß√µes:**
1. Verificar qual processo consome CPU
   ```bash
   top -b -n 1 | head -20
   ```

2. Identificar container problem√°tico
   ```bash
   docker stats --no-stream | sort -k 3 -h -r
   ```

3. Poss√≠veis causas:
   - Prometheus com muitas m√©tricas
   - Grafana com dashboards complexos
   - Database com queries lentas
   - Aplica√ß√£o com memory leak

4. Resolu√ß√£o:
   - Se Prometheus: aumentar scrape_interval
   - Se Grafana: simplificar dashboards
   - Se banco: otimizar queries
   - Se app: restart container

**SLA:** Investigar dentro de 15 minutos

---

### HighMemoryUsage

**Severidade:** warning
**Servi√ßo:** node-exporter
**Condi√ß√£o:** Memory > 85% por >1 minuto
**Significado:** Host est√° ficando sem mem√≥ria

**A√ß√µes:**
1. Verificar uso de mem√≥ria
   ```bash
   free -h
   docker stats --no-stream
   ```

2. Container com maior uso
   ```bash
   docker stats --no-stream | sort -k 4 -h -r
   ```

3. Poss√≠veis causas:
   - Database cache muito grande
   - Prometheus retention muito longo
   - Memory leak em aplica√ß√£o
   - Redis usando muita mem√≥ria

4. Resolu√ß√£o curta:
   ```bash
   # Reiniciar container problem√°tico
   docker compose restart <service>
   ```

5. Resolu√ß√£o longa:
   - Aumentar mem√≥ria do host
   - Reduzir retention em Prometheus
   - Otimizar database queries
   - Implementar memory limits

**SLA:** Investigar dentro de 10 minutos

---

### LowDiskSpace

**Severidade:** critical
**Servi√ßo:** node-exporter
**Condi√ß√£o:** Disco < 10% livre
**Significado:** Disco est√° quase cheio - risco de falha total

**A√ß√µes Imediatas:**
1. Verificar uso de disco
   ```bash
   df -h
   du -sh /var/lib/docker/volumes/infra-default-*
   ```

2. Identificar maior consumidor
   ```bash
   docker system df
   docker volume ls -q | xargs -I {} sh -c 'echo {} && du -sh /var/lib/docker/volumes/{}/_data 2>/dev/null'
   ```

3. Liberar espa√ßo (em ordem de seguran√ßa):
   ```bash
   # Op√ß√£o 1: Prune unused images (seguro)
   docker image prune -a

   # Op√ß√£o 2: Reduce Prometheus retention (requer restart)
   # Edit prometheus.yml, change retention to 7d
   docker compose restart prometheus

   # Op√ß√£o 3: Delete old backups (verificar primeiro!)
   ls -lh /backups/ | tail -10
   ```

4. Monitor ap√≥s a√ß√£o
   ```bash
   df -h
   docker compose ps  # Verificar se todos containers est√£o up
   ```

**SLA:** A√ß√£o imediata - pode afetar todos os servi√ßos

**Preventivo:**
- Monitorar tend√™ncia de crescimento
- Planejar expans√£o de disco
- Implementar logs rotation

---

## Database Alerts

### MongoDBDown

**Severidade:** critical
**Servi√ßo:** mongo
**Condi√ß√£o:** MongoDB Exporter unreachable >1 minuto
**Significado:** MongoDB n√£o est√° respondendo ou exporter falhou

**A√ß√µes:**
1. Verificar status
   ```bash
   docker compose ps mongo
   docker compose logs mongo --tail=20
   ```

2. Testar conectividade
   ```bash
   docker exec infra-default-mongo \
     mongosh --eval "db.adminCommand('ping')"
   ```

3. Se n√£o responder:
   ```bash
   docker compose restart mongo
   ```

4. Se problema persistir:
   ```bash
   # Verificar volume
   docker volume inspect infra-default-mongo_data

   # Reset completo
   docker compose down mongo
   docker volume rm infra-default-mongo_data
   docker compose up -d mongo
   ```

**SLA:** Resolver dentro de 5 minutos

---

### PostgreSQLDown

**Severidade:** critical
**Servi√ßo:** postgres
**Condi√ß√£o:** PostgreSQL Exporter unreachable >1 minuto
**Significado:** PostgreSQL n√£o est√° respondendo

**A√ß√µes:**
1. Verificar status
   ```bash
   docker compose ps postgres
   docker compose logs postgres --tail=20
   ```

2. Testar conectividade
   ```bash
   docker exec infra-default-postgres \
     psql -U $POSTGRES_USER -c "SELECT 1"
   ```

3. Se n√£o responder:
   ```bash
   docker compose restart postgres
   ```

4. Se problema persistir:
   ```bash
   docker compose down postgres
   docker volume rm infra-default-postgres_data
   docker compose up -d postgres
   ```

**SLA:** Resolver dentro de 5 minutos

---

### MySQLDown

**Severidade:** critical
**Servi√ßo:** mysql
**Condi√ß√£o:** MySQL Exporter unreachable >1 minuto
**Significado:** MySQL n√£o est√° respondendo

**A√ß√µes:**
1. Verificar status
   ```bash
   docker compose ps mysql
   docker compose logs mysql --tail=20
   ```

2. Testar conectividade
   ```bash
   docker exec infra-default-mysql \
     mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SELECT 1"
   ```

3. Se n√£o responder:
   ```bash
   docker compose restart mysql
   ```

4. Se problema persistir:
   ```bash
   docker compose down mysql
   docker volume rm infra-default-mysql_data
   docker compose up -d mysql
   ```

**SLA:** Resolver dentro de 5 minutos

---

### RedisDown

**Severidade:** critical
**Servi√ßo:** redis
**Condi√ß√£o:** Redis Exporter unreachable >1 minuto
**Significado:** Redis n√£o est√° respondendo

**A√ß√µes:**
1. Verificar status
   ```bash
   docker compose ps redis
   docker compose logs redis --tail=20
   ```

2. Testar conectividade
   ```bash
   docker exec infra-default-redis redis-cli ping
   ```

3. Se n√£o responder:
   ```bash
   docker compose restart redis
   ```

4. Se problema persistir:
   ```bash
   docker compose down redis
   docker volume rm infra-default-redis_data
   docker compose up -d redis
   ```

**SLA:** Resolver dentro de 5 minutos

---

## Monitoring & Network Alerts

### BlackboxDown

**Severidade:** warning
**Servi√ßo:** blackbox-exporter
**Condi√ß√£o:** Blackbox Exporter unreachable >1 minuto
**Significado:** Exporter de healthcheck externo offline

**A√ß√µes:**
1. Verificar status
   ```bash
   docker compose ps blackbox-exporter
   docker compose logs blackbox-exporter --tail=20
   ```

2. Restart
   ```bash
   docker compose restart blackbox-exporter
   ```

**SLA:** Investigar dentro de 10 minutos

---

### BlackboxICMPDown

**Severidade:** warning
**Condi√ß√£o:** ICMP probes failing
**Significado:** Conectividade de rede degradada (ICMP/ping)

**A√ß√µes:**
1. Testar conectividade ICMP
   ```bash
   ping -c 5 8.8.8.8
   ```

2. Verificar network
   ```bash
   docker network inspect infra-default-shared-net
   ```

**SLA:** Investigar dentro de 15 minutos

---

## Message Queue Alerts

### RabbitMQDown

**Severidade:** warning
**Condi√ß√£o:** RabbitMQ Exporter unreachable
**Significado:** RabbitMQ n√£o est√° respondendo

**A√ß√µes:**
```bash
docker compose restart rabbitmq
docker compose logs rabbitmq --tail=20
```

**SLA:** Investigar dentro de 10 minutos

---

### RabbitMQQueueTooLarge

**Severidade:** warning
**Condi√ß√£o:** Fila RabbitMQ > limite configurado
**Significado:** Mensagens acumulando, poss√≠vel slowdown em consumers

**A√ß√µes:**
1. Listar filas
   ```bash
   docker exec infra-default-rabbitmq rabbitmqctl list_queues
   ```

2. Investigar consumers
   ```bash
   docker exec infra-default-rabbitmq rabbitmqctl list_consumers
   ```

3. Escalar consumers se necess√°rio

**SLA:** Investigar dentro de 15 minutos

---

### RabbitMQConsumersZero

**Severidade:** warning
**Condi√ß√£o:** Nenhum consumer ativo em alguma fila
**Significado:** Mensagens podem n√£o ser processadas

**A√ß√µes:**
1. Verificar consumers
   ```bash
   docker exec infra-default-rabbitmq rabbitmqctl list_consumers
   ```

2. Verificar aplica√ß√£o
   ```bash
   docker compose logs | grep -i "rabbitmq\|consumer"
   ```

3. Reiniciar servi√ßo consumer se necess√°rio

**SLA:** Investigar dentro de 10 minutos

---

## Authentication Alerts

### KeycloakDown

**Severidade:** warning
**Condi√ß√£o:** Keycloak unreachable
**Significado:** Servi√ßo de autentica√ß√£o offline

**A√ß√µes:**
```bash
docker compose restart keycloak
docker compose logs keycloak --tail=20
```

**SLA:** Investigar dentro de 15 minutos

---

## Alert Response Workflow

```
Alert Fires
    ‚Üì
Prometheus detects ‚Üí Rule evaluation
    ‚Üì
Alert goes to Alertmanager
    ‚Üì
Alertmanager groups alerts (by alertname)
    ‚Üì
Matches routing rules in alertmanager.yml
    ‚Üì
Sends notification (email via Mailhog)
    ‚Üì
Wait 10s minimum before next notification
    ‚Üì
Repeat every 1 hour until resolved
```

---

## Alert Severity Levels

| Severidade | SLA | A√ß√£o |
|-----------|-----|------|
| **critical** | 5 min | Investigar imediatamente, pode afetar opera√ß√µes |
| **warning** | 15 min | Investigar quando poss√≠vel, n√£o cr√≠tico |
| **test** | N/A | Ignorar, apenas testes |

---

## Common False Positives

### Intermittent "Down" Alerts

**Causa:** Exporter temporariamente unreachable (network blip)
**Solu√ß√£o:** Normal, n√£o agir se normaliza em 1-2 minutos

### High Memory After Restart

**Causa:** Database cache aquecendo
**Solu√ß√£o:** Normal, normaliza em 5-10 minutos

### High CPU on Update

**Causa:** Processos de sistema ou container initialization
**Solu√ß√£o:** Normal, normaliza em 1-2 minutos

---

## Disabling Alerts (Temporarily)

```bash
# Via Alertmanager API
curl -X POST http://localhost:9093/api/v1/alerts/silence \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [
      {"name": "alertname", "value": "HighMemoryUsage", "isRegex": false}
    ],
    "duration": "1h",
    "comment": "Maintenance window"
  }'
```

---

## Monitoring Dashboard

**Grafana:** http://localhost:3001
- Dashboard: "Alerts Overview"
- Shows: Firing alerts, history, trends

**Alertmanager UI:** http://localhost:9093
- Shows: Current alerts, routing, groups, silences

---

**Last Updated:** 2025-11-08
**Maintainer:** Operations Team
